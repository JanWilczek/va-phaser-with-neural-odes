
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Diode Clipper Modeling}
\label{chap:diode_clipper}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Training Data}
\label{sec:diode_clipper_training_data}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The dataset to be used for the diode clipper modeling consisted of 7 minutes and 59 seconds of guitar and bass recordings from \cite{Abesser2013} and \cite{Kehling2014} respectively. The amount of guitar recordings was roughly the same as the amount of bass recordings and their ordering was arbitrary. All recordings were single-channel and used \SI{44100}{Hz} sampling rate. 1 minute and 29 seconds (approximately 20\%) of these were used as the test set. Care was taken so that the test file begins with silence. The remaining data was split into the validation set (1 minute and 18 seconds) and the train set (5 minutes and 12 seconds) according to the 80:20 rule. The input were raw recordings and the target distorted signal was synthesized from a SPICE model of the circuit with the schematic from \Figure{fig:diode_clipper_circuit} and parameter values from \Table{tab:diode_clipper_element_parameters}. For the simulation, LTspice XVII by Analog Devices was used \cite{LTspice}. The synthesized target data sounds realistically and was previously used with success in \cite{Wright2019}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Training}
\label{sec:diode_clipper_training}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The training procedure was as follows. Firstly, the dataset was loaded, then the network architecture was initialized, and the training parameters were set (optimization algorithm, hyperparameters, learning rate schedule, loss function). Then the proper training was run for a fixed number of epochs. After each epoch, the validation loss was computed. Finally, after finishing the last epoch, the test set was processed and the model's output along with the final loss value were recorded.

The training set was split into half-second segments. These segments were randomly shuffled at the beginning of each epoch and split into minibatches of a predetermined size.

A single epoch consisted of processing the minibatches of segments in chunks (subsegments) of a given length. After each subsegment, the gradient of the loss with respect to the network parameters was calculated using the \ac{BPTT}. Then, the gradient step was performed using the Adam optimizer \cite{Kingma2017}, the computational graph was discarded and the next subsegment processed. After each minibatch, the learning rate scheduler performed its step (if such a scheduler was set). When an epoch ended, model parameters were stored by overwriting the previously saved ones. Additionally, the training session kept track of the model which performed best on the validation set.

The number of epochs in training was determined by manually observing the validation loss curve. If the validation loss reached a plateau below 1\% of the loss, the training was deemed successful.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Compared Models}
\label{sec:diode_clipper_models}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

For the assessment of how well ODENet can model the diode clipper, two derivative networks were chosen: $2 \times 100 \times 100 \times 1$ (ODENet100) and $2 \times 9 \times 9 \times 1$ (ODENet9), both with the \ac{ReLU} nonlinearity. The latter is the smallest derivative network that has reached validation loss smaller than 1\%. Both were trained and tested with the forward Euler scheme. Additionally, the latter was trained and tested with the implicit Adams scheme and \ac{SELU} nonlinearity.

For benchmark, the following architectures were chosen:
\begin{itemize}
    \item \ac{STN} $2 \times 4 \times 4 \times 4 \times 1$ with $\tanh$ nonlinearity and biases enabled only in the second layer from \cite{Parker2019}, 
    \item \ac{RINN} with \ac{RK}4 integration parameters and time step fixed to 1 (\ac{RINN}4) using a bilinear block with 6 units in each fully connected layer (BB6) from \cite{Ouala2019}, and 
    \item \ac{LSTM} with 8 hidden units and a $8 \times 1$ output \ac{MLP} (mapping the hidden states to an output sample) from \cite{Wright2019} (\ac{LSTM}8).
\end{itemize}

All of the architectures were trained on audio data at \SI{44100}{Hz} sampling rate.

Each model is listed with its number of parameters,
hyperparameter values during training, training time (in epochs and hours), and teacher forcing curriculum in \Table{tab:diode_clipper_models_data}. The curriculum descriptions mean the following: "never" does not use teacher forcing, "always" provides true output for each minibatch, "bernoulli" uses scheduled sampling, where the probability of using teacher forcing decreases with each epoch. 
\begin{table}[]
    \caption{Compared network architectures for diode clipper modeling}
    \input{tex/chapter_03/diode_clipper_models_data.tex}
    \label{tab:diode_clipper_models_data}
\end{table}

Each of these models was tested on previously unseen data at \SI{44100}{Hz} (same as training data), \SI{22050}{Hz}, \SI{48000}{Hz}, and \SI{192000}{Hz} sampling rates. The goal in using various sampling rates at test time was to analyze the presence of aliasing in the output. During test, \ac{STN} and ODENet models were informed about the new value of the times step, whereas \ac{LSTM}8 and \ac{RINN}4 were not. Test loss $\mathcal{E}(y, \hat{y})$ given by \Equation{eq:final_loss_function}, \ac{segSNR} given by \Equation{eq:seg_snr}, and \ac{ODG} described in \Section{subsec:va_evaluation}.
were calculated for each model and test sampling rate.

All models were tested on a one long sequence but the implicit Adams scheme consistently diverged in this test setting. Therefore, its tests were conducted using segments of 22050 samples which were concatenated afterwards.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Results and Discussion}
\label{sec:diode_clipper_results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In \Table{tab:diode_clipper_results} the test results of the compared models are shown. Loss and \ac{segSNR} are rounded to two significant digits, whereas \ac{ODG} is rounded to three significant digits. The best results are given in bold (lowest loss, highest \ac{segSNR} and \ac{ODG}). The models are separately evaluated in terms of the learned model quality (test sampling rate equal to training sampling rate) and performance at sampling rates different from the training sampling rate. 

% ODENet @ 22kHz test sampling rate goes into self oscillations

\begin{table}[]
    \caption{Test results of the diode clipper models.}
    \input{tex/chapter_03/diode_clipper_results_table.tex}
    \label{tab:diode_clipper_results}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Model Quality}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The model quality in terms of the loss function is very good for all compared architectures (less than 1\% loss). In casual listening, only \ac{RINN}4 sounds differently from the target (despite the low test loss value). In terms of metrics, \ac{LSTM} outperformed all other models, even multiple times larger ODENet100(FE).

Results comparable to the established \ac{LSTM} and \ac{STN} architectures prove that ODENet is suitable for diode clipper modeling. Although the diode clipper is quite simple to learn, the results provide a proof of concept that ODENet, extended to incorporate an input signal into its processing, could be used for \ac{VA} modeling.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Behavior at Unseen Sampling Rates}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

For \SI{22050}{Hz} test sampling rate (half of the training sampling rate), the lowest loss value was obtained for \ac{RINN}4. The second best result, \ac{LSTM}8, obtained the highest \ac{segSNR} and \ac{ODG} and sounds most accurately in casual listening. The output of \ac{STN} as well as all ODENet models contain high amount of digital distortion. Moreover, ODENet100(FE) and ODENet9(IA) fall into self-oscillations, i.e., contain a constant high-frequency component even when input signal is very quiet. Although the digital distortion sounds unpleasantly, it is a sign that the networks learned the true derivative (or residual in the case of \ac{STN}) contained in the data. The derivative networks probably output a correct value but given a larger time step without any additional tuning, the produced sound contains components beyond the Nyquist frequency, which causes aliasing in the output sound. Contrary to the casually perceived quality of \ac{LSTM}, all produced outputs received equally low \ac{ODG}. It seems that models that were not informed about the time step significantly outperformed the informed ones. Thus, they are more suitable for processing at sampling rates lower than the training sampling rate.

For \SI{48000}{Hz} test sampling rate, again \ac{LSTM} outperformed other models in the analyzed metrics, yet all models had test loss value smaller than 1\%. In casual listening, there is not much difference between the model outputs.

For \SI{192000}{Hz} test sampling rate, the advantage of the ODENet becomes clear with the largest model having the lowest loss value and biggest \ac{segSNR} and \ac{ODG}. The smaller ODENet models obtained just slightly worse results, especially given their significantly smaller number of parameters. As could be expected, architectures that take the sampling rate into account during processing (\ac{STN}, ODENet) obtained lower loss value and significantly higher \ac{ODG}. The only exception is the low \ac{ODG} of ODENet9(IA), probably because of zero-clicks resulting from all-zero initialization at the beginning of each segment. A slower but more accurate implicit solver obtained loss comparable to a hundred times larger architecture using an explicit scheme. However, all test loss values are below 1\% and \ac{segSNR} is relatively high.

All in all, ODENet architectures proved their suitability for processing at sampling rates higher than training sampling rate. It was possible even for the smallest considered parametrization (9 hidden units) and the simplest numerical scheme, i.e., the forward Euler. 

Although there was an advantage of the implicit solver over the explicit one in terms of the loss value but at the cost of doubled time needed to process the same amount of data, as indicated by the training time in \Table{tab:diode_clipper_models_data}. For the diode clipper model, it does not seem beneficial to increase the derivative network's capacity beyond the necessary minimum (here: 9 hidden units). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Convergence Speed}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
    \centering
    \input{figures/tikz/diode_clipper_validation_curves/diode_clipper_validation_curves.tex}
    \caption{Convergence curves of two diode clipper models.}
    \label{fig:diode_clipper_validation_curves}
\end{figure}

Looking at how the validation loss of ODENet and LSTM changes during training, as shown in \Figure{fig:diode_clipper_validation_curves}, one can observe that the former takes many more epochs to get below 1\% than the latter. This may be explained by the nature of the \ac{LSTM} architecture, where the constant-error carousel ensures that error gets propagated even for very long time lags \cite{Hochreiter1997}. Slow convergence can be seen as a disadvantage of the ODENet framework, slowing down the derivative network architecture exploration or hyperparameter search. Increasing the learning rate of the ODENet, for example, by using the one-cycle learning rate schedule, has proven itself beneficial but must be used with care; if the learning rate is too large, ODENet diverges.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Comparison to Numerical Solvers}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To get a sense of how a learned derivative performs in comparison to closed-form solution, a numerical solver was run on the diode clipper \ac{ODE} as presented in \Equation{eq:diode_clipper_equation} using the test set at \SI{44100}{Hz} sampling rate. To avoid aliasing, we oversampled the data before supplying it to the solver. The initial experiments showed, that scaling the input data by a factor of 20 significantly decreased the loss. After the solver completed the processing, its output was rescaled so as to minimize the \ac{ESR} as given by \Equation{eq:esr}. The loss function-informed scalings were supposed to compensate for the fact that the solver or the \ac{ODE} could not learn the proper scaling in a training phase as the \ac{ANN} models did and that scaling in itself should not determine the loss value of the solver's output. 

The results of solving the diode clipper \ac{ODE} with the \ac{BDF} solver at 8x sampling rate and with the \ac{RK}4 solver at 30x sampling rate are presented in \Table{tab:diode_clipper_ode_solvers}. The metrics were computed with respect to the target of the test set used to evaluate the \ac{ANN} models.

\begin{table}[]
    \centering
    \caption{Results of solving the diode clipper \ac{ODE}.}
    \input{tex/chapter_03/diode_clipper_ode_solvers.tex}
    \label{tab:diode_clipper_ode_solvers}
\end{table}

One can observe that the numerical solutions of the \ac{ODE} achieved less than 1\% of the test loss. At the same time they obtained much lower values of \ac{segSNR} and \ac{ODG} than the ODENet models from \Table{tab:diode_clipper_results}. Moreover, the data input to the ODENet did not need any oversampling, because having a non-aliased dataset rules out the possibility of the derivative network learn an aliasing behavior. Thus, apart from not having to derive the \ac{ODE} and learning it from data instead, ODENet additionally does not need its input to be oversampled.

It needs to be stressed again, that data used to conduct training and test of the \acp{ANN} was synthesized as well using LTspice \cite{LTspice}. There is, however, no reason to doubt that the networks would perform equally well on discretized, measured data.

\newcommand{\subfigureWidth}{0.4\textwidth}
\newcommand{\subfigureScale}{0.8}

\begin{figure}
    \centering
    \begin{subfigure}{0.9\textwidth}
        \centering
        \scalebox{\subfigureScale}{\input{figures/tikz/diode_clipper_derivative_visualization/analytical_derivative.tex}}
        \caption{After \Equation{eq:diode_clipper_equation}. Every magnitude $m$ is transformed to $10 \log_{10}(m)$.}
    \end{subfigure}
    \begin{subfigure}{0.9\textwidth}
        \centering
        \scalebox{\subfigureScale}{\input{figures/tikz/diode_clipper_derivative_visualization/FE100-ode_derivative.tex}}
        \caption{ODENet100(FE)}
    \end{subfigure}
    \begin{subfigure}{\subfigureWidth}
        \scalebox{0.75}{\input{figures/tikz/diode_clipper_derivative_visualization/FE9-ode_derivative.tex}}
        \caption{ODENet9(FE)}
    \end{subfigure}
    \begin{subfigure}{\subfigureWidth}
        \scalebox{0.76}{\input{figures/tikz/diode_clipper_derivative_visualization/IA-ode_derivative.tex}}
        \caption{ODENet9(IA)}
    \end{subfigure}
    \caption{Magnitude of the derivative (analytical or learned) for given input and output signal values of the diode clipper.}
    \label{fig:diode_clipper_derivative_visualizations}
\end{figure}

In \Figure{fig:diode_clipper_derivative_visualizations} the derivative function given by \Equation{eq:diode_clipper_equation} and learned by ODENet100(FE), ODENet9(FE), and ODENet9(IA)  are visualized. The $x$-axis spans the input signal range, the $y$-axis spans the output signal range, and with color the magnitude of the derivative function in this two-dimensional space is indicated. Additionally, to enable at all an inspection of the analytical derivative, its magnitude $m$ was transformed to $10 \log_{10}(m)$. While there is not much difference between the derivative function of the both derivative networks, they are significantly different from the analytical form. This difference probably comes from time discretization of the training data. Additionally, obtained results are on a par with the visualizations shown in \cite{Parker2019}.
