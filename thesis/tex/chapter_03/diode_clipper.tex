
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Diode Clipper Modeling}
\label{chap:diode_clipper}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Training Data}
\label{sec:diode_clipper_training_data}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The dataset to be used for the diode clipper modeling consisted of 7 minutes and 59 seconds of guitar and bass recordings from \cite{Abesser2013} and \cite{Kehling2014} respectively. The amount of guitar recordings was roughly the same as the amount of bass recordings and their ordering was arbitrary. All recordings were single-channel and used \SI{44100}{Hz} sampling rate. 1 minute and 29 seconds (approximately 20\%) of these were used as the test set. Care was taken so that the test file begins with silence. The remaining data was split into the validation set (1 minute and 18 seconds) and the train set (5 minutes and 12 seconds) according to the 80:20 rule. The input were raw recordings and the target distorted signal was synthesized from a SPICE model of the circuit with the schematic from \Figure{fig:diode_clipper_circuit} and parameter values from \Table{tab:diode_clipper_element_parameters}. For the simulation, LTspice XVII by Analog Devices was used \cite{LTspice}. The synthesized target data sounds realistically and was previously used with success in \cite{Wright2019}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Training}
\label{sec:diode_clipper_training}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The training procedure was as follows. Firstly, the dataset was loaded, then the network architecture was initialized, and the training parameters were set (optimization algorithm, hyperparameters, learning rate schedule, loss function). Then the proper training was run for a fixed number of epochs. After each epoch, the validation loss was computed. Finally, after finishing the last epoch, the test set was processed and the model's output along with the final loss value were recorded.

The training set was split into half-second segments. These segments were randomly shuffled at the beginning of each epoch and split into minibatches of a predetermined size.

A single epoch consisted of processing the minibatches of segments in chunks (subsegments) of a given length. After each subsegment, the gradient of the loss with respect to the network parameters was calculated using the \ac{BPTT}. Then, the gradient step was performed using the Adam optimizer \cite{Kingma2017}, the computational graph was discarded and the next subsegment processed. After each minibatch, the learning rate scheduler performed its step (if such a scheduler was set). When an epoch ended, model parameters were stored by overwriting the previously saved ones. Additionally, the training session kept track of the model which performed best on the validation set.

The number of epochs in training was determined by manually observing the validation loss curve. If the validation loss reached a plateau below 1\% of the loss, the training was deemed successful.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Compared Models}
\label{sec:diode_clipper_models}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

For the assessment of how well ODENet can model the diode clipper, two derivative networks were chosen: $2 \times 100 \times 100 \times 1$ (ODENet100) and $2 \times 9 \times 9 \times 1$ (ODENet9), both with the \ac{ReLU} nonlinearity. The latter is the smallest derivative network that has reached validation loss smaller than 1\%. Both were trained and tested with the forward Euler scheme. Additionally, the latter was trained and tested with the implicit Adams scheme and \ac{SELU} nonlinearity.

For benchmark, the following architectures were chosen:
\begin{itemize}
    \item \ac{STN} $2 \times 4 \times 4 \times 4 \times 1$ with $\tanh$ nonlinearity and biases enabled only in the second layer from \cite{Parker2019}, 
    \item \ac{RINN} with \ac{RK}4 integration parameters (\ac{RINN}4) using a bilinear block with 6 units in each fully connected layer (BB6) from \cite{Ouala2019}, and 
    \item \ac{LSTM} with 8 hidden units and a $8 \times 1$ output \ac{MLP} (mapping the hidden states to an output sample) from \cite{Wright2019} (\ac{LSTM}8).
\end{itemize}

All of the architectures were trained on audio data at \SI{44100}{Hz} sampling rate.

Each model is listed with its number of parameters,
% TODO: computational complexity in processing (ops/sample, GFLOPS), 
hyperparameter values during training, training time (in epochs and hours), and teacher forcing curriculum in \Table{tab:diode_clipper_models_data}. The curriculum descriptions mean the following: "never" does not use teacher forcing, "always" provides true output for each minibatch, "bernoulli" uses scheduled sampling, where the probability of using teacher forcing decreases with each epoch. 
\begin{table}[]
    \caption{Compared network architectures for diode clipper modeling}
    \input{tex/chapter_03/diode_clipper_models_data.tex}
    \label{tab:diode_clipper_models_data}
\end{table}

Each of these models was tested on previously unseen data at \SI{44100}{Hz} (same as training data), \SI{22050}{Hz}, \SI{48000}{Hz}, and \SI{192000}{Hz} sampling rates. The goal in using various sampling rates at test time was to analyze the presence of aliasing in the output. Test loss $\mathcal{E}(y, \hat{y})$ given by \Equation{eq:final_loss_function}, \ac{segSNR} given by \Equation{eq:seg_snr}, and \ac{ODG} described in 
% TODO: Add section including the ODG
were calculated for each model and test sampling rate.

All models were tested on a one long sequence but the implicit Adams scheme consistently diverged in this test setting. Therefore, its tests were conducted using segments of 22050 samples which were concatenated afterwards.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Results and Discussion}
\label{sec:diode_clipper_results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In \Table{tab:diode_clipper_results} the test results of the compared models are shown. Loss and \ac{segSNR} are rounded to two significant digits, whereas \ac{ODG} is rounded to three significant digits. The best results are given in bold (lowest loss, highest \ac{segSNR} and \ac{ODG}). The models are separately evaluated in terms of the learned model quality (test sampling rate equal to training sampling rate) and performance at sampling rates different from the training sampling rate. 

% ODENet @ 22kHz test sampling rate goes into self oscillations

\begin{table}[]
    \caption{Test results of the diode clipper models.}
    \input{tex/chapter_03/diode_clipper_results_table.tex}
    \label{tab:diode_clipper_results}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Model Quality}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The model quality in terms of the loss function is very good for all compared architectures. In casual listening, only the \ac{RINN}4 model sounds differently from the target (despite the low test loss value). Their \ac{segSNR} values are comparatively low with the ODENet9(IA) receiving slightly better results than others. Its forward Euler version, ODENet9(FE), has the highest \ac{ODG}. Somewhat surprisingly, the smaller ODENet models outperformed the larger one in terms of \ac{segSNR} and \ac{ODG}. This could be explained by the fact that limiting the model capacity has a regularizing effect \cite{Goodfellow-et-al-2016}.

Results better or comparable to the established \ac{LSTM} and \ac{STN} architectures prove that ODENet is suitable for diode clipper modeling. Although the diode clipper is quite simple to learn, the results provide a proof of concept that ODENet, extended to incorporate an input signal into its processing, could be used for \ac{VA} modeling.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Behavior at Unseen Sampling Rates}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

For \SI{22050}{Hz} test sampling rate (half of the training sampling rate), the lowest loss value was obtained for \ac{LSTM} with 8 hidden units. The second best result (\ac{RINN}4) was already an order of magnitude worse, with the remaining models obtaining over a hundred times worse results. The highest \ac{segSNR} was achieved by \ac{RINN}4. \ac{LSTM} sounds best and quite accurately in casual listening. However, the output of \ac{STN} as well as all ODENet models contain high amount of digital distortion. Moreover, ODENet100(FE) and ODENet9(IA) fall into self-oscillations, i.e., contain a constant high-frequency component even when input signal is very quiet. Although the digital distortion sounds unpleasantly, it is a sign that the networks learned the true derivative (or residual in the case of \ac{STN}) contained in the data. The derivative networks probably output a correct value but given a larger time step without any additional tuning, the produced sound contains components beyond the Nyquist frequency, which causes aliasing in the output sound. Contrary to the casually perceived quality of \ac{LSTM}, all produced outputs received equally low \ac{ODG}. 

For \SI{48000}{Hz} test sampling rate, again \ac{LSTM} received the best loss value, yet all models had test loss value smaller than 1\%. In terms of \ac{segSNR} and \ac{ODG}, smaller ODENet models again took the lead as for \SI{44100}{Hz} sampling rate. In casual listening, there is not much difference between the model outputs.

For \SI{192000}{Hz} test sampling rate, the advantage of the ODENet becomes clear with the larger model having the lowest loss value and smaller models outperforming all the other ones with respect to \ac{segSNR} and \ac{ODG}. As could be expected, architectures that take sampling rate into account during processing (\ac{STN}, ODENet) obtained lower loss value and significantly higher \ac{ODG}. The output of \ac{LSTM} sounds significantly distorted in comparison to other architectures, suggesting that this architecture may not be the most optimal option for processing at sampling rates higher than in training. 

All in all, ODENet architectures outperformed other models in terms of \ac{segSNR} and \ac{ODG} for all test sampling rates higher than training sampling rate. What is more, they continued to have loss value smaller than 1\% for those sampling rates. It was possible even for the simplest numerical scheme, i.e., the forward Euler. Therefore, ODENet is suitable for training at one sampling rate and processing at a higher one.

There was no clear advantage of the implicit solver over the explicit one. On the contrary, the implicit solver took over twice the time to process the same amount of data as the explicit solver did, as indicated in \Table{tab:diode_clipper_models_data}.For the diode clipper model, it does not seem beneficial to increase the derivative network's capacity beyond the necessary minimum (here: 9 hidden units). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Convergence Speed}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
    \centering
    \input{figures/tikz/diode_clipper_validation_curves/diode_clipper_validation_curves.tex}
    \caption{Convergence curves of the diode clipper models.}
    \label{fig:diode_clipper_validation_curves}
\end{figure}

Looking at how validation loss of ODENet and LSTM changes during training, as shown in \Figure{fig:diode_clipper_validation_curves}, one can observe that the former takes much more epochs to get below 1\% than the latter. This may be explained by the nature of the \ac{LSTM} architecture, where the constant-error carousel ensures that error gets propagated even for very long time lags \cite{Hochreiter1997}. Slow convergence can be seen as a disadvantage of the ODENet framework, slowing down the derivative network architecture exploration or hyperparameter search. Increasing the learning rate of the ODENet, for example using one-cycle learning rate schedule, has proven itself beneficial but must be used with care; if the learning rate is too large, ODENet diverges.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Comparison to Numerical Solvers}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


