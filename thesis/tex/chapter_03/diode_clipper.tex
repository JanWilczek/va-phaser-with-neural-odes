
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Diode Clipper Modeling}
\label{chap:diode_clipper}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Training Data}
\label{sec:diode_clipper_training_data}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The dataset to be used for the diode clipper modeling consisted of 7 minutes and 59 seconds of guitar and bass recordings from \cite{Abesser2013} and \cite{Kehling2014} respectively. The amount of guitar recordings was roughly the same as the amount of bass recordings and their ordering was arbitrary. All recordings were single-channel and used \SI{44100}{Hz} sampling rate. 1 minute and 29 seconds (approximately 20\%) of these were used as the test set. Care was taken so that the test file begins with silence. The remaining data was split into the validation set (1 minute and 18 seconds) and the train set (5 minutes and 12 seconds) according to the 80:20 rule. The input were raw recordings and the target distorted signal was synthesized from a SPICE model of the circuit with the schematic from \Figure{fig:diode_clipper_circuit} and parameter values from \Table{tab:diode_clipper_element_parameters}. For the simulation, LTspice XVII by Analog Devices was used \cite{LTspice}. The synthesized target data sounds realistically and was previously used with success in \cite{Wright2019}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Training}
\label{sec:diode_clipper_training}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The training procedure was as follows. Firstly, the dataset was loaded, then the network architecture was initialized, and the training parameters were set (optimization algorithm, hyperparameters, learning rate schedule, loss function). Then the proper training was run for a fixed number of epochs. After each epoch, the validation loss was computed. Finally, after finishing the last epoch, the test set was processed and the model's output along with the final loss value were recorded.

The training set was split into half-second segments. These segments were randomly shuffled at the beginning of each epoch and split into minibatches of a predetermined size.

A single epoch consisted of processing the minibatches of segments in chunks (subsegments) of a given length. After each subsegment, the gradient of the loss with respect to the network parameters was calculated using the \ac{BPTT}. Then, the gradient step was performed using the optimizer, the computation graph discarded and the next subsegment processed. After each minibatch, the learning rate scheduler performed its step (if such a scheduler was set). When an epoch has ended, model parameters were stored by overwriting the previously saved ones. Additionally, the training session kept track of the model which performed best on the validation set.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Compared Models}
\label{sec:diode_clipper_models}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% TODO: List the best validation loss models
For the assessment of how well ODENet can model the diode clipper two derivative networks were chosen: and $2 \times 100 \times 100 \times 1$ and $2 \times 9 \times 9 \times 1$, both with \ac{ReLU} nonlinearity. The latter is the smallest derivative network that had validation loss smaller than 1\%. Both were trained and tested with the forward Euler scheme. Additionally, the latter was trained and tested with the implicit Adams scheme.

For comparison, the \ac{STN} $2 \times 4 \times 4 \times 4 \times 1$ architecture with $\tanh$ nonlinearity and biases only in one layer from \cite{Parker2019}, \ac{RINN} with \ac{RK}4 weights using a bilinear block with 6 units in each fully connected layer, and \ac{LSTM} with 8 hidden units and a $8 x 1$ output \ac{MLP} (mapping the hidden states to an output sample) from \cite{Wright2019} were chosen. All of these models were trained on audio data at \SI{44100}{Hz} sampling rate.

Each of the model was listed with its number of parameters, computational complexity in processing (ops/sample, GFLOPS), hyperparameter values during traning, training time (in epochs and hours), and best validation loss epoch.  %TODO Add table
\begin{table}[]
    \input{tex/chapter_03/diode_clipper_models_data.tex}
\end{table}

Each of these models was tested on previously unseen data at \SI{44100}{Hz} (same as training data), \SI{22050}{Hz}, \SI{48000}{Hz}, and \SI{192000}{Hz} sampling rates. The goal in using various sampling rates at test time was to analyze the presence of aliasing in the output. Test loss, \ac{segSNR}, \ac{fw-segSNR}, and \ac{ODG} are reported for each model. 

It must be noted that all models were tested on a one long audio recording but implicit Adams scheme consistently diverged in this test setting. Therefore, its tests were conducted using segments of 22050 samples which were concatenated afterwards.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Results and Discussion}
\label{sec:diode_clipper_results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% TODO: Present the test results, evaluation results and discuss them
In \Table{tab:diode_clipper_results} the test results of the compared models are shown. The models were trained on \SI{44100}{Hz} data but tested using 4 different sampling rates: \SI{44100}{Hz}, \SI{220500}{Hz}, \SI{48000}{Hz}, and \SI{192000}{Hz}. For each of the sampling rates, the test loss $\mathcal{E}(y, \hat{y})$ given by \Equation{eq:final_loss_function}, \ac{segSNR} given by \Equation{eq:seg_snr}, and \ac{ODG} described in 
% TODO: Add section including the ODG
are displayed with \ac{ODG} being rounded to two decimal places. The best results are given in bold (lowest loss, highest \ac{segSNR} and \ac{ODG}). The models are separately evaluated in terms of the learned model quality (test sampling rate equal to training sampling rate) and performance at sampling rates different from the training sampling rate. 

% ODENet @ 22kHz test sampling rate goes into self oscillations

\begin{table}[]
    \caption{Test results of the diode clipper models.}
    \input{tex/chapter_03/diode_clipper_results_table.tex}
    \label{tab:diode_clipper_results}
\end{table}

The model quality in terms of the loss function is very good for all compared models. In casual listening only the \ac{RINN}4 model sounds differently from the target (despite the low test loss value). Their \ac{segSNR} values are comparatively low with the implicit Adams ODENet receiving slightly better results. Its forward Euler version has the highest \ac{ODG}. Somewhat surprisingly, the smaller ODENet models outperform the larger one. This could be explained by the fact that limiting the model capacity has a regularizing effect \cite{Goodfellow-et-al-2016}.

For \SI{22050}{Hz} test sampling rate (half of the training sampling rate), the lowest loss value was obtained for \ac{LSTM} with 8 hidden units. The second best result (\ac{RINN}4) was already an order of magnitude worse, with the remaining models having over a hundred times worse result. The highest \ac{segSNR} was achieved by \ac{STN}. The outputs of both these models as well as the output of \ac{RINN}4 sound accurately in casual listening. However, the output of all ODENet models sounds as if it contained high amount of digital aliasing. Moreover, FE hidden100 and IA hidden9 fall into self-oscillations, i.e., contain a constant high-frequency component. Although it sounds unpleasantly, it is a sign that the network learned the true derivative contained in the data. The derivative network outputs a correct value but given a larger time step without any additional tuning, the produced sound contains components beyond the Nyquist frequency, which alias the output sound. Contrary to the casually perceived quality of \ac{LSTM}, \ac{STN}, and \ac{RINN}4, all produced outputs received equally low \ac{ODG}. 

For \SI{48000}{Hz} test sampling rate, again \ac{LSTM} received the best loss value, yet all models had loss smaller than 1\%. However, in terms of \ac{segSNR} and \ac{ODG}, smaller ODENet models took the lead with \ac{STN} being equally good. Interestingly, IA hidden9 had best \ac{segSNR} and worst \ac{ODG} score. In casual listening, there is not much difference between the model outputs.

For \SI{192000}{Hz} test sampling rate, the advantage of the ODENet becomes clear with small models outperforming all the other ones with respect to \ac{segSNR} and \ac{ODG}. 
