\section{Phaser Modeling}

This section presents the results of phaser modeling in a manner analogous to the diode clipper results presented in the previous section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Training Data}
\label{sec:phaser_training_data}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The training data consisted solely of guitar recordings taken from the Fraunhofer IDMT database \cite{Kehling2014}. Some of the recordings were additionally distorted using a software distortion plug-in. The purpose of applying additional distortion was to make the effect of phaser application more audible thanks to the harmonic-rich spectrum of distorted signals. The underlying assumption was that the network would learn better from these more pronounced parts. Additionally, they should make the evaluation task easier during listening tests. 

The acoustic, electric, and distorted guitar recordings were evenly split among the training, validation, and test sets. The training set was 5 minutes 44 seconds long, the validation set was 1 minute 6 seconds long, and the test set was 1 minute 49 seconds long. Audio data was single-channel at \SI{44100}{Hz} sampling rate.

The dataset was synthesized using the digital model of a phaser from \cite{Kiiski2016} with the feedback turned off. The purpose of using synthesized data instead of recorded data was to provide a ground truth LFO signal. This was the approach taken for the initial validation in \cite{Wright2020}. If a recording had been used, we would have needed to estimate the LFO signal, which could obscure the analysis. The LFO signal used for the synthesis was a rectified sine at \SI{17}{Hz}. Again, since the purpose of the training was to validate if the ODENet architecture could be applied to phaser modeling, only one type and rate of the LFO signal were used. If the answer was positive, more LFO waveforms and frequencies would be used (probably involving some not seen during training at test time). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Training}
\label{sec:phaser_training}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[t]
    \begin{subfigure}{0.33\textwidth}
        \centering
        \scalebox{\scaleboxsize}{\input{figures/tikz/spectral_distances/LSTM16_L1_STFT/test_output_stft.tex}}
    \end{subfigure}
    \begin{subfigure}{0.28\textwidth}
        \centering
        \scalebox{\scaleboxsize}{\input{figures/tikz/spectral_distances/LSTM16_L2_STFT/test_output_stft.tex}}
    \end{subfigure}
    \begin{subfigure}{0.34\textwidth}
        \centering
        \scalebox{0.61}{\input{figures/tikz/spectral_distances/LSTM16_rmsLSD_STFT/test_output_stft.tex}}
    \end{subfigure}
    \caption{Spectrogram of the outputs of the baseline \ac{LSTM}16 model for training losses measuring three different average distance measures in the \ac{STFT} domain. From left to right: $L_1$ distance, $L_2$ distance, root mean square \ac{LSD}.}
    \label{fig:spectral_distances}
\end{figure}

The training procedure was identical to the one for the diode clipper as described in \Section{sec:diode_clipper_training}. The only difference was that the validation loss was computed every 10 epochs instead of every epoch. All models were trained and tested on data at \SI{44100}{Hz} sampling rate.

The loss functions used for the training and the validation were the average distance measures in the time-frequency domain as described in \Section{sec:loss_functions}. Different distance measures used in the loss function lead to different characteristics of the trained models. The difference between the baseline models trained with these distances can be seen in \Figure{fig:spectral_distances}. The average root mean square \ac{LSD} loss results in the most pronounced notches in the magnitude frequency spectrum of the output. However, this observation does not correlate with the validation error as measured by the $\mathcal{E}$ loss from \Equation{eq:final_loss_function}; the model trained using \ac{LSD} has the highest validation error of the three. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Compared Models}
\label{sec:phaser_models}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In the case of the phaser models, only the ODENet framework was compared to the baseline: an \ac{LSTM} from \cite{Wright2020} with 16 memory cells (\ac{LSTM}16). For the derivative network a sufficiently large \ac{MLP} was chosen. Its dimensionality was $M + 2 \times 30 \times 60 \times 60\times 60 \times 30\times M$, where $M$ was the manually set size of the state vector. The nonlinearity used was \ac{SELU}. To prevent divergence, we added a weight decay term to the loss function as a regularizer.

\begin{table}[]
    \centering
    \input{tex/chapter_03/phaser_models_data.tex}
    \caption{Compared network architectures for phaser modeling}
    \label{tab:phaser_models_data}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Results and Discussion}
\label{sec:phaser_results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The goal of phaser modeling was not only to compare ODENet to the approach taken in \cite{Wright2020} but also to verify the assumption on the state augmentation. The assumption was that the derivative network can use the unobserved entries in the state vector to store information helpful in obtaining more accurate results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{The Impact of State Augmentation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The effect of augmenting the state vector from 1 to 18 entries when training with the average $L_1$ distance in the \ac{STFT} domain can be seen in \Figure{fig:state_augmentation}. Although no target was provided for the 17 latent entries, the network used them to store meaningful information for each time point. The observed improvement is over two-fold. Therefore, it may be beneficial to augment the state, even if the training signal is not provided for all of its entries. However, the improvement was not observed when $L_2$ or \ac{LSD} distances were used.

\begin{figure}
    \centering
    \input{figures/tikz/state_augmentation/state_augmentation.tex}
    \caption{Average $L_1$ loss of ODENet in the \ac{STFT} domain on the validation set with and without state augmentation.}
    \label{fig:state_augmentation}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Comparison to the Baseline}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Test results of phaser modeling are shown in \Table{tab:phaser_results}, where \ac{LSD} was defined in \Equation{eq:log_spectral_distance}, \ac{segSNR} was defined in \Equation{eq:seg_snr}, and \ac{PEAQ} was explained in \Section{subsec:va_evaluation}. The \ac{LSD}  loss was chosen for the final training and comparison, because during a visual inspection of the magnitude \ac{STFT} it resulted in the best match of phaser notches in the output signal of the baseline model. It was assumed that it would enable effective learning for ODENet as well.

\begin{table}[]
    \centering
    \input{tex/chapter_03/phaser_results_table.tex}
    \caption{Test results of the phaser models.}
    \label{tab:phaser_results}
\end{table}

In terms of numerical measures, \ac{LSTM}16 significantly outperformed ODENet. The validation curves of both models can be seen in \Figure{fig:phaser_lstm_vs_fe}. \ac{LSTM}16 has smaller dynamics of learning, which could point to the fact that it is more suited for modeling the phaser.

\begin{figure}
    \centering
    \input{figures/tikz/phaser_lstm_vs_fe/phaser_lstm_vs_fe.tex}
    \caption{Average \ac{LSD} of the compared models on the validation set.}
    \label{fig:phaser_lstm_vs_fe}
\end{figure}

The results in the \ac{STFT} domain can be seen in \Figure{fig:phaser_test_spectrograms}. The notches learned by \ac{LSTM}16 closely match those of the target. The notches learned by ODENet are almost invisible (although they are present at closer inspection). It must also be stated that ODENet always diverged during test, what can be seen in the range of values in the colorbar.

\newcommand{\scaleboxsizee}{0.8}
\begin{figure}
    \centering
    \begin{subfigure}{0.7\textwidth}
        \centering
        \scalebox{0.81}{\input{figures/tikz/phaser_test_spectrograms/FameSweetToneOffNoFb-test-target_stft.tex}}
    \end{subfigure}
    \begin{subfigure}{0.7\textwidth}
        \centering
        \scalebox{\scaleboxsizee}{\input{figures/tikz/phaser_test_spectrograms/LSTM16_log_spectral_distance_stft.tex}}
    \end{subfigure}
    \begin{subfigure}{0.7\textwidth}
        \centering
        \scalebox{\scaleboxsizee}{\input{figures/tikz/phaser_test_spectrograms/FE_log_spectral_distance_stft.tex}}
    \end{subfigure}
    \caption{A fragment of the magnitude \ac{STFT} of the models' test output. (Top) Target. (Middle) \ac{LSTM}16. (Bottom) ODENet (state size 36).}
    \label{fig:phaser_test_spectrograms}
\end{figure}

In casual listening, \ac{LSTM}16 sounds indistinguishable from the target. On the contrary, the output of ODENet does not sound as processed with phaser at all. The gentle notches that can be seen under a close inspection of ODENet output spectrograms are not audible.

Ultimately, we were not able to achieve a better performance of ODENet on the phaser dataset. This suggests that ODENet may not be able to learn the instantaneous derivative of the phaser system.
